apiVersion: apps/v1
kind: Deployment
metadata:
  name: end-to-end-pipeline
  labels:
    app: pipeline
spec:
  replicas: 1   # One pod to minimize cost
  selector:
    matchLabels:
      app: pipeline
  template:
    metadata:
      labels:
        app: pipeline
    spec:
      initContainers:
        - name: create-airflow-db
          image: postgres:14
          command: ["/bin/bash", "-c"]
          args:
            - |
              echo "Checking if Airflow role exists..."
              psql -h data-pipeline-rds.cet8668qsrqm.us-east-1.rds.amazonaws.com -U dandy -d postgres -tc "SELECT 1 FROM pg_roles WHERE rolname='airflow';" | grep -q 1 || \
              psql -h data-pipeline-rds.cet8668qsrqm.us-east-1.rds.amazonaws.com -U dandy -d postgres -c "CREATE ROLE airflow WITH LOGIN PASSWORD 'airflow';"

              echo "Checking if Airflow DB exists..."
              psql -h data-pipeline-rds.cet8668qsrqm.us-east-1.rds.amazonaws.com -U dandy -d postgres -tc "SELECT 1 FROM pg_database WHERE datname='airflow';" | grep -q 1 || \
              psql -h data-pipeline-rds.cet8668qsrqm.us-east-1.rds.amazonaws.com -U dandy -d postgres -c "CREATE DATABASE airflow;"
              psql -h data-pipeline-rds.cet8668qsrqm.us-east-1.rds.amazonaws.com -U dandy -d postgres -c "GRANT ALL PRIVILEGES ON DATABASE airflow TO airflow;"

          env:
            - name: PGPASSWORD
              value: "dandy123"

        - name: init-airflow-schema
          image: cutedandelion/airflow-pipeline:latest
          command: ["/bin/bash", "-c"]
          args:
            - |
              echo "Running DB migration..."
              airflow db migrate
          env:
            - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN
              value: "postgresql+psycopg2://dandy:dandy123@data-pipeline-rds.cet8668qsrqm.us-east-1.rds.amazonaws.com:5432/airflow"

      containers:
        # Airflow UI / DAG runner
        - name: airflow
          image: cutedandelion/airflow-pipeline:latest
          command: ["airflow", "standalone"]
          ports:
            - containerPort: 5000
          env:
            - name: AIRFLOW__CORE__EXECUTOR
              value: "LocalExecutor"
            - name: AIRFLOW__CORE__LOAD_EXAMPLES
              value: "False"
            - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN
              value: "postgresql+psycopg2://dandy:dandy123@data-pipeline-rds.cet8668qsrqm.us-east-1.rds.amazonaws.com:5432/airflow"
            - name: AIRFLOW__WEBSERVER__WEB_SERVER_HOST
              value: "0.0.0.0"
            - name: AIRFLOW__WEBSERVER__WEB_SERVER_PORT
              value: "8080"
            - name: AIRFLOW__API__HOST
              value: "0.0.0.0"
            - name: AIRFLOW__API__PORT
              value: "5000"
          volumeMounts:
            - name: airflow-logs
              mountPath: /opt/airflow/logs

        # Zookeeper for Kafka
        - name: zookeeper
          image: confluentinc/cp-zookeeper:7.5.11
          ports:
            - containerPort: 2181
          env:
            - name: ZOOKEEPER_CLIENT_PORT
              value: "2181"

        # Kafka broker
        - name: kafka
          image: confluentinc/cp-kafka:7.5.11
          ports:
            - containerPort: 9092
              name: kafka
            - containerPort: 9999
              name: jmx
          env:
            - name: KAFKA_ZOOKEEPER_CONNECT
              value: "localhost:2181"
            - name: KAFKA_LISTENERS
              value: "PLAINTEXT://0.0.0.0:9092"
            - name: KAFKA_ADVERTISED_LISTENERS
              value: "PLAINTEXT://localhost:9092"
            - name: JMX_PORT
              value: "9999"
            - name: KAFKA_OPTS
              value: "-Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.port=9999 -Dcom.sun.management.jmxremote.rmi.port=9999 -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false"

        #  JMX exporter sidecar
        - name: jmx-exporter
          image: bitnami/jmx-exporter:latest
          args:
            - "/etc/jmx-exporter/config.yml"
          ports:
            - containerPort: 5556
              name: metrics
          volumeMounts:
            - name: jmx-config
              mountPath: /etc/jmx-exporter

        # Kafka client - to run producer script
        - name: kafka-client
          image: python:3.10
          command: ["/bin/bash", "-c"]
          args:
            # Install kafka-python once, then sleep forever
            - |
              pip install --no-cache-dir kafka-python && \
              sleep infinity

        # Spark job runner
        - name: spark
          image: cutedandelion/spark-pipeline:latest
          ports:
            - containerPort: 4040
          env:
            - name: SPARK_UI_PORT
              value: "4040"
            - name: SPARK_MASTER_PORT
              value: "7077"
            - name: AWS_ACCESS_KEY
              valueFrom:
                secretKeyRef:
                  name: minio-credentials
                  key: MINIO_ACCESS_KEY
            - name: AWS_SECRET_KEY
              valueFrom:
                secretKeyRef:
                  name: minio-credentials
                  key: MINIO_SECRET_KEY
            - name: RAW_DATA_PATH
              value: "s3a://data-pipeline-data-bucket-de0af943/raw-data/streaming_raw/"
            - name: ANOMALY_DATA_PATH
              value: "s3a://data-pipeline-data-bucket-de0af943/processed-data/streaming_anomalies/"
        # Dev storage
        - name: mongodb
          image: mongo:latest
          ports:
            - containerPort: 27017

        # Metrics DB
        - name: influxdb
          image: influxdb:latest
          ports:
            - containerPort: 8086

      # Volumes (simple and ephemeral)
      volumes:
        - name: airflow-logs
          emptyDir: {}
        - name: jmx-config
          configMap:
            name: kafka-jmx-config


---

apiVersion: v1
kind: ConfigMap
metadata:
  name: kafka-jmx-config
data:
  config.yml: |
    startDelaySeconds: 0
    hostPort: "localhost:9999"
    lowercaseOutputName: true
    lowercaseOutputLabelNames: true
    rules:
      - pattern: "kafka.server<type=(.+), name=(.+)PerSec\\w*><>Count"
        name: kafka_server_$1_$2_total
        type: COUNTER
        help: Kafka server $1 $2 per second count
      - pattern: "kafka.server<type=(.+), name=(.+)><>Value"
        name: kafka_server_$1_$2
        type: GAUGE
        help: Kafka server $1 $2
      - pattern: "kafka.network<type=(.+), name=(.+)><>Value"
        name: kafka_network_$1_$2
        type: GAUGE
        help: Kafka network $1 $2
      - pattern: ".*"

